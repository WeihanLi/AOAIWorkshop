{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic Kernel**\n",
    "\n",
    "Semantic Kernel 是一个轻量级的开源框架，通过 Semantic Kernel 您可以快速使用不同编程语言(C#/Python/Java)结合 LLMs(OpenAI、Azure OpenAI、Hugging Face 等模型) 构建智能应用。在我们进入生成式人工智能后，人机对话的方式有了很大的改变，我们用自然语言就可以完成与机器的对话，门槛降低了非常多。结合提示工程和大型语言模型，我们可以用更低的成本完成不同的业务。但如何把提示工程以及大模型引入到工程上？我们就需要一个像 Semantic Kernel 的框架作为开启智能大门的基础。在 2023 年 5 月，微软 CTO Kevin Scott 就提出了 Copilot Stack 的概念，人工智能编排就是核心。 Semantic Kernel 具备和 LLMs 以及各种提示工程/软件组成的插件组合的能力，因此也被看作 Copilot Stack 的最佳实践。通过 Semantic Kernel，你可以非常方便地构建基于 Copilot Stack 的解决方案，而且对于传统工程，也可以无缝对接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Kernel 的特点**\n",
    "\n",
    "强大的插件 - 你可以通过结合自定义/预定义的插件解决智能业务的问题。让传统的代码和智能插件一起工作灵活地接入到应用场景，简化传统应用向智能化转型的过程。\n",
    "\n",
    "多模型支持 - 为您的智能应用配置“大脑”，可以是来自 Azure OpenAI Service , 也可以是 OpenAI ，以及来自 Hugging Face 上的各种离线模型。通过链接器你可以快速接入不同的“大脑”，让您的应用更智能更聪明。\n",
    "\n",
    "各式各样的链接器 - 链接器除了链接“大脑”外，还可以链接如向量数据库，各种商业软件，不同的业务中间件，让更多的业务场景进入智能成为可能\n",
    "\n",
    "开发便捷 - 简单易用，开发人员零成本入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Kernel 的缺点**\n",
    "\n",
    "毕竟 LLMs 还在不停发展，有很多新模型的加入，也有很多新的功能，以及新的概念引入。Semantic Kernel， LangChain 等开源框架都在努力适应这个新的摩尔定律，但版本的迭代会有不确定的更改。所以在使用的时候，开发者需要多留意对应 GitHub Repo 上的变更日志。\n",
    "\n",
    "还有 Semantic Kernel 需要兼顾多个编程语言，所以进度也是不一致，也会导致 Semantic Kernel 在不同技术栈人群的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Kernel vs LangChain**\n",
    "\n",
    "我们没法不去作出一些客观的比较，毕竟 LangChain 拥有更多的使用群体。无可否认在落地场景上，LangChain 现在比 Semantic Kernel 更多，特别在入门参考例子上。我们来个更为全面的比较：\n",
    "\n",
    "LangChain 基于 Python 和 Javascript的开源框架，包含了众多预制组件，开发者可以无需多写提示工程就可以完成智能应用的开发。特别在复杂的应用场景，开发人员可以快速整合多个预定义的组件来综合完成。在开发角度，更适合具备数据科学或则人工智能基础的开发人员。\n",
    "\n",
    "Semantic Kernel 您可以基于 C#, Python, Java 的开源框架。更大的优势在工程化。毕竟它更像一个编程范式，传统开发人员可以很快掌握该框架进行应用开发，而且可以更好结合自定义的插件和提示工程完成企业的定制化业务智能化工作。\n",
    "\n",
    "两者有很多共通点，都还在版本迭代，我们需要基于团队结构，技术栈，应用场景作出选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.9.4b1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (3.9.3)\n",
      "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: grpcio>=1.50.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (1.62.1)\n",
      "Requirement already satisfied: jinja2<4.0.0,>=3.1.3 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel) (3.1.3)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.6.0 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel) (1.6.0)\n",
      "Requirement already satisfied: numpy>=1.25 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel) (1.26.4)\n",
      "Requirement already satisfied: openai>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (1.14.2)\n",
      "Requirement already satisfied: openapi_core<0.20,>=0.18 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (0.19.0)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pybars4<0.10.0,>=0.9.13 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (0.9.13)\n",
      "Requirement already satisfied: pydantic<3,>=2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (2.6.3)\n",
      "Requirement already satisfied: python-dotenv<2.0.0,>=1.0.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (1.0.1)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from semantic-kernel) (2023.12.25)\n",
      "Requirement already satisfied: scipy>=1.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (4.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2<4.0.0,>=3.1.3->semantic-kernel) (2.1.5)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.2->semantic-kernel) (4.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (0.26.0)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/codespace/.local/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.9.0)\n",
      "Requirement already satisfied: isodate in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /home/codespace/.local/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (4.21.1)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.3.2)\n",
      "Requirement already satisfied: more-itertools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: parse in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi_core<0.20,>=0.18->semantic-kernel) (3.0.1)\n",
      "Requirement already satisfied: chardet>=3.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/codespace/.local/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (23.2)\n",
      "Requirement already satisfied: PyMeta3>=0.5.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pybars4<0.10.0,>=0.9.13->semantic-kernel) (0.5.1)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=2->semantic-kernel) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic<3,>=2->semantic-kernel) (2.16.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (2023.12.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.31.1)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.17.1)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/codespace/.local/lib/python3.10/site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (6.0.1)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from jsonschema-path<0.4.0,>=0.3.1->openapi_core<0.20,>=0.18->semantic-kernel) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /home/codespace/.local/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.20,>=0.18->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.20,>=0.18->semantic-kernel) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.2->semantic-kernel) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/codespace/.local/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (2.0.7)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (0.2.8)\n",
      "Requirement already satisfied: qdrant_client in /usr/local/python/3.10.13/lib/python3.10/site-packages (1.8.0)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qdrant_client) (1.62.1)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qdrant_client) (1.62.1)\n",
      "Requirement already satisfied: httpx>=0.14.0 in /home/codespace/.local/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (0.26.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/codespace/.local/lib/python3.10/site-packages (from qdrant_client) (1.26.4)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qdrant_client) (2.8.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qdrant_client) (2.6.3)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from qdrant_client) (2.0.7)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /usr/local/python/3.10.13/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (68.2.2)\n",
      "Requirement already satisfied: anyio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.0.2)\n",
      "Requirement already satisfied: idna in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/codespace/.local/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/codespace/.local/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/codespace/.local/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (4.9.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (4.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/codespace/.local/lib/python3.10/site-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install semantic-kernel -U\n",
    "! pip install qdrant_client -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from dotenv import load_dotenv\n",
    "import semantic_kernel.connectors.ai.open_ai as skaoai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic Kernel 中的 Kernel**\n",
    "\n",
    "如果把 Semantic Kernel 看作是 Copilot Stack 最佳实践，那 Kernel 就是 AI 编排的中心，在官方文档中也有所提及。通过 Kernel 可以和不同插件，服务，日志以及不同的模型链接在一起。所有 Semantic Kernel 的应用都从 Kernel 开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sk_kernel](../../../imgs/ChatWithYourData/kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://kinfeyaoai.openai.azure.com/'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_service = skaoai.AzureChatCompletion(\"GPT3Model\",endpoint,api_key=api_key,api_version = \"2023-12-01-preview\")\n",
    "embedding_gen = skaoai.AzureTextEmbedding(\"EmbeddingModels\", endpoint,api_key=api_key,api_version = \"2023-12-01-preview\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.add_service(chat_service)\n",
    "\n",
    "\n",
    "kernel.add_service(embedding_gen)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Semantic Kernel 中，我们有不同的插件，用户可以使用预定义的插件，也可以使用自定义的插件。想了解更多可以关注下一章的内容，我们会详细讲述插件的使用。该例子，我们使用的是自定义插件，已经在 plugins 目录下了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_plugin = \"./plugins\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **什么是插件**\n",
    "\n",
    "Semantic Kernel 的一大特点是拥有强大的插件，通过结合自定义/预定义的插件解决智能业务的问题。让传统的代码和智能插件一起工作灵活地接入到应用场景简化传统应用向智能化转型的过程。\n",
    "\n",
    "我们知道 LLMs 本来的数据是有时间限制的，如果要增加实时内容或者企业化的知识是有相当大的缺陷。OpenAI 通过插件将 ChatGPT 连接到第三方应用程序。 这些插件使 ChatGPT 能够与开发人员定义的 API 进行交互，从而增强 ChatGPT 的功能并允许有更广泛的操作，如：\n",
    "\n",
    "检索实时信息，例如，体育赛事比分、股票价格、最新新闻等。\n",
    "\n",
    "检索知识库信息， 例如，公司文档、个人笔记等。\n",
    "\n",
    "协助用户进行相关操作，例如，预订航班、订餐等。\n",
    "\n",
    "Semantic Kernel 遵循 OpenAI 的插件的插件规范，可以很方便地接入和导出插件(如基于 Bing, Microsoft 365, OpenAI 的插件)，这样可以让开发人员很简单地调用不同的插件服务。除了兼容 OpenAI 的插件外，Semantic Kernel 内也有属于自己插件定义的方式。不仅可以在规定模版格式上定义 Plugins, 更可以在函数内定义 Plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **通过模版定义插件**\n",
    "\n",
    "我们知道通过提示工程可以和 LLMs 进行对话。对于一个企业或者创业公司，我们在处理业务时，可能不是一个提示工程，可能需要有针对提示工程的合集。我们可以把这些针对业务能力的提示工程集放到 Semantic Kernel 的插件集合内。对于结合提示工程的插件，Semantic Kernel 有固定的模版，提示工程都放在 skprompt.txt 文件内，而相关参数设置都放在 config.json 文件内。最后的文件结构式这样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_plugin = kernel.import_plugin_from_prompt_directory(base_plugin , \"FilePlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_files = os.listdir(\"./data/notes\")\n",
    "transcrips_files = os.listdir(\"./data/transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kblist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error parsing XML of prompt: not well-formed (invalid token): line 3, column 50\n",
      "Error parsing XML of prompt: not well-formed (invalid token): line 3, column 50\n",
      "Error parsing XML of prompt: not well-formed (invalid token): line 3, column 50\n"
     ]
    }
   ],
   "source": [
    "for f in nodes_files:\n",
    "    file = open(\"./data/notes/\"+f, \"r\") \n",
    "    content = file.read()\n",
    "    notesFunc = files_plugin[\"Notes\"]\n",
    "    result = await kernel.invoke(notesFunc, sk.KernelArguments(input=content))\n",
    "    # print(result.result)\n",
    "    json_result = json.loads(str(result))\n",
    "    kblist.append(json_result)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kb': 'History of machine learning',\n",
       "  'content': \"In this lesson, we will walk through the major milestones in the history of machine learning and artificial intelligence. The history of artificial intelligence (AI) as a field is intertwined with the history of machine learning, as the algorithms and computational advances that underpin ML fed into the development of AI. It is useful to remember that, while these fields as distinct areas of inquiry began to crystallize in the 1950s, important algorithmic, statistical, mathematical, computational and technical discoveries predated and overlapped this era. In fact, people have been thinking about these questions for hundreds of years: this article discusses the historical intellectual underpinnings of the idea of a 'thinking machine.' Notable discoveries include Bayes Theorem and its predecessors, Least Square Theory, Markov Chains, Perceptron, Nearest Neighbor, Backpropagation, and Recurrent Neural Networks. Alan Turing and the Dartmouth Summer Research Project in 1956 were pivotal moments in the history of ML and AI. The 'golden years' from the 1950s to the mid '70s saw optimism and advancements in natural language processing, search, and micro-worlds. The 'AI Winter' in the mid '70s to 1980 saw a decline in funding and confidence in the field. The 1980s saw the rise of expert systems and increasing attention to neural networks. The 'AI Chill' from 1987 to 1993 was marked by the democratization of computing. From 1993 to 2011, ML and AI saw a new era of growth with the increase in data and compute power. Today, ML and AI are pervasive in our lives and call for careful understanding and regulation. The future remains uncertain, but it is important to gain a better understanding of these computer systems and the software and algorithms that they run.\"},\n",
       " {'kb': 'Techniques of Machine Learning',\n",
       "  'content': 'The process of building, using, and maintaining machine learning models involves several steps. These steps include deciding on a question, collecting and preparing data, choosing a training method, training the model, evaluating the model, parameter tuning, and making predictions. Feature selection and extraction, visualizing data, and splitting the dataset are important pre-building tasks. Building a model involves deciding on a training method, training the model, and evaluating its quality. Parameter tuning can be done to improve the model. Finally, predictions can be made using new data. Overall, the process of machine learning requires careful consideration of data, training methods, and model evaluation.'},\n",
       " {'kb': 'Introduction to machine learning',\n",
       "  'content': \"Welcome to this course on classical machine learning for beginners! Whether you're completely new to this topic, or an experienced ML practitioner looking to brush up on an area, we're happy to have you join us! We want to create a friendly launching spot for your ML study and would be happy to evaluate, respond to, and incorporate your feedback. Before starting with this curriculum, you need to have your computer set up and ready to run notebooks locally. Familiarize yourself with Scikit-learn, a set of ML libraries that we reference in these lessons. The term 'machine learning' is one of the most popular and frequently used terms of today. The mechanics of machine learning, however, are a mystery to most people. For a machine learning beginner, the subject can sometimes feel overwhelming. Therefore, it is important to understand what machine learning actually is, and to learn about it step by step, through practical examples. Machine learning automates the process of pattern-discovery by finding meaningful insights from real-world or generated data. It has proven itself to be highly valuable in business, health, and financial applications, among others. In the near future, understanding the basics of machine learning is going to be a must for people from any domain due to its widespread adoption.\"}]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f in transcrips_files:\n",
    "    file = open(\"./data/transcripts/\"+f, \"r\") \n",
    "    content = file.read()\n",
    "    transcripsFunc = files_plugin[\"Transcrips\"]\n",
    "    result = await kernel.invoke(transcripsFunc, sk.KernelArguments(input=content))\n",
    "    # print(result.result)\n",
    "    json_result = json.loads(str(result))\n",
    "    for item in json_result:\n",
    "        kblist.append(item)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kb': 'History of machine learning',\n",
       "  'content': \"In this lesson, we will walk through the major milestones in the history of machine learning and artificial intelligence. The history of artificial intelligence (AI) as a field is intertwined with the history of machine learning, as the algorithms and computational advances that underpin ML fed into the development of AI. It is useful to remember that, while these fields as distinct areas of inquiry began to crystallize in the 1950s, important algorithmic, statistical, mathematical, computational and technical discoveries predated and overlapped this era. In fact, people have been thinking about these questions for hundreds of years: this article discusses the historical intellectual underpinnings of the idea of a 'thinking machine.' Notable discoveries include Bayes Theorem and its predecessors, Least Square Theory, Markov Chains, Perceptron, Nearest Neighbor, Backpropagation, and Recurrent Neural Networks. Alan Turing and the Dartmouth Summer Research Project in 1956 were pivotal moments in the history of ML and AI. The 'golden years' from the 1950s to the mid '70s saw optimism and advancements in natural language processing, search, and micro-worlds. The 'AI Winter' in the mid '70s to 1980 saw a decline in funding and confidence in the field. The 1980s saw the rise of expert systems and increasing attention to neural networks. The 'AI Chill' from 1987 to 1993 was marked by the democratization of computing. From 1993 to 2011, ML and AI saw a new era of growth with the increase in data and compute power. Today, ML and AI are pervasive in our lives and call for careful understanding and regulation. The future remains uncertain, but it is important to gain a better understanding of these computer systems and the software and algorithms that they run.\"},\n",
       " {'kb': 'Techniques of Machine Learning',\n",
       "  'content': 'The process of building, using, and maintaining machine learning models involves several steps. These steps include deciding on a question, collecting and preparing data, choosing a training method, training the model, evaluating the model, parameter tuning, and making predictions. Feature selection and extraction, visualizing data, and splitting the dataset are important pre-building tasks. Building a model involves deciding on a training method, training the model, and evaluating its quality. Parameter tuning can be done to improve the model. Finally, predictions can be made using new data. Overall, the process of machine learning requires careful consideration of data, training methods, and model evaluation.'},\n",
       " {'kb': 'Introduction to machine learning',\n",
       "  'content': \"Welcome to this course on classical machine learning for beginners! Whether you're completely new to this topic, or an experienced ML practitioner looking to brush up on an area, we're happy to have you join us! We want to create a friendly launching spot for your ML study and would be happy to evaluate, respond to, and incorporate your feedback. Before starting with this curriculum, you need to have your computer set up and ready to run notebooks locally. Familiarize yourself with Scikit-learn, a set of ML libraries that we reference in these lessons. The term 'machine learning' is one of the most popular and frequently used terms of today. The mechanics of machine learning, however, are a mystery to most people. For a machine learning beginner, the subject can sometimes feel overwhelming. Therefore, it is important to understand what machine learning actually is, and to learn about it step by step, through practical examples. Machine learning automates the process of pattern-discovery by finding meaningful insights from real-world or generated data. It has proven itself to be highly valuable in business, health, and financial applications, among others. In the near future, understanding the basics of machine learning is going to be a must for people from any domain due to its widespread adoption.\"},\n",
       " {'kb': 'Decide if AI is the right approach',\n",
       "  'content': \"traditional software is well suited to solve problems where the solution can be described as a formal set of rules in contrast AI shines in solving problems where the solution can be extracted from data many of the problems we encountered in our daily life can be efficiently solved with traditional programming if an engineer can break up the solution of a problem and Define it using precise rules then traditional programming is a great tool to use but many of the problems we encounter in our day-to-day aren't quite as easy to Define as a set of rules thankfully for many of those problems we have access to plenty of real life data containing useful information which means that AI can help us find a solution one good example is translating from one language to another writing a set of rules that full encodes all the parallels between two languages is not easy but there are many examples of translation online so AI has been able to do a much better job of translation than previous attempts so our first step when we're starting a new project should be to analyze the problem and determine which technique is best to solve it if you're able to obtain plenty of data that contains useful information about your solution then AI is a promising approach once you decided that\"},\n",
       " {'kb': 'Collect and prepare data',\n",
       "  'content': \"AI is the right method for you you need to collect and prepare your data for example you may need to normalize it or convert it to a different form or remove rows that are missing certain Fields once your data is clean you need to decide about which aspects of your data or features you're going to use as input to your prediction and which feature you want to predict for example if you have medical data you may decide to use features that describe the patient's medical history as input and a chance of a particular disease as the output feature you want to predict and finally you need to split your data into training and test sets a usual split is 80 for your training data and 20 for test\"},\n",
       " {'kb': 'Train your model',\n",
       "  'content': \"next you need to choose a machine learning algorithm which you'll learn a lot about in the coming videos if you're undecided between a few good algorithms you may want to try them all and see which one performs best then you need to train your model using the training set you collected earlier and the algorithm you chose training a model may take a while especially if the model is large\"},\n",
       " {'kb': 'Evaluate your model',\n",
       "  'content': \"once the model is trained you can test it using the test data set that you split earlier it's important that you test the algorithm with data that it hasn't seen during training to ensure that it generalizes well to new scenarios\"},\n",
       " {'kb': 'Tune the hyperparameters',\n",
       "  'content': \"some algorithms contain hyperparameters which are settings that control key aspects of their inner workings choosing good hyperparameters is important because they can make a big difference in your results if you want to be systematic about your hyperparameter search you can write code that tries lots of different combinations and helps you discover the best values for your data once you get good test results it's time to see how well your model performs within the context of its intended use\"},\n",
       " {'kb': 'Test the model in the real world',\n",
       "  'content': \"time to see how well your model performs within the context of its intended use for example this could involve collecting live data from a sensor and using it to make predictions or deploying a model to a few users of your application if it all looks good then you're ready to release it to production and enjoy its benefits\"},\n",
       " {'kb': 'Introducing ML for Beginners',\n",
       "  'content': 'based on the free open source 26 lesson ml for beginners curriculum from Microsoft'},\n",
       " {'kb': 'The difference between AI and ML',\n",
       "  'content': 'AI is a science of getting machines to accomplish tasks that typically require human level intelligence'},\n",
       " {'kb': \"What you'll learn in this course\",\n",
       "  'content': \"going to cover what we call classical machine learning you'll learn some Core Concepts of ml a bit of History statistical techniques like regression classification clustering and more\"},\n",
       " {'kb': 'Advanced Techniques',\n",
       "  'content': \"the concepts you'll learn here will serve you well as you progress to more\"},\n",
       " {'kb': 'Why study Machine Learning',\n",
       "  'content': \"Hot Topic because it's solving complex real-world problems in so many areas\"},\n",
       " {'kb': 'Alan Turing and the Turing test',\n",
       "  'content': \"intelligence really begun in 1950s though it's based on mathematical and statistical developments over many centuries Alan Turing is credited with helping to lay the foundation for the concept of a machine they can think in his quest to Define machine intelligence he achieved a crucial Milestone by creating the Turing test in 1950. in this test an interrogator questions both a human and a computer and tries to determine which one is which if the interrogator cannot tell the difference then the computer can be considered intelligent\"},\n",
       " {'kb': 'The Dartmouth Summer Research Project on AI',\n",
       "  'content': 'artificial intelligence was coined with a small group of scientists gathered at Dartmouth College in the U.S for an event called the summer research project on artificial intelligence this conference was the birth of the field of research we know as AI'},\n",
       " {'kb': 'The golden years of AI',\n",
       "  'content': 'Golden Ears of AI optimism ran high in the hope that AI could solve many problems in 1967 Marvin Minsky the co-founder of the mitai lab stated confidently and incorrectly that within a generation the problem of creating artificial intelligence will substantially be solved natural language processing research flourished search was refined and made more powerful and the concept of micro worlds was created where simple tasks were completed using plain language instructions research was well funded by government agencies advances were made in computation and algorithms and prototypes of intelligent machines were built some of these machines include shaky the robot who could maneuver and decide how to perform tasks intelligently Eliza and gnarly charabot that could converse with people and act as a primitive therapist Blocksworld an example of a micro world where blogs get could be stacked and sorted and decision-making experiments could be tested by the mid-1970s it had'},\n",
       " {'kb': 'The AI winter',\n",
       "  'content': 'become apparent that the complexity of making intelligent machines had been understated and that its promise had been overblown compute power was too limited there was a lack of data to train and test AIS and there were questions around the ethics of introducing AI systems like the therapist Eliza into society funding dried up and confidence in the field slowed marking the beginning of what is called an AI winter in the 1980s as computers became more powerful expert'},\n",
       " {'kb': 'Resurgence and fall of AI for expert systems',\n",
       "  'content': 'systems became more successful there was a Resurgence in optimism about AI as businesses found practical applications of these rule-based inference systems by the late 80s it was becoming apparent that expert systems had become too specialized and were unlikely to achieve machine intelligence the rise of personal computers also competed with these large specialized centralized systems this led to another chill in the AI field things began to change in the mid-1990s as compute and storage capabilities grew exponentially making it possible to process much larger data'},\n",
       " {'kb': 'Growth in AI driven by more data and more powerful hardware',\n",
       "  'content': 'sets than ever before the rise of the internet and the popularity of smartphones both contributed to increasing amounts of data a new experiments in machine learning became possible throughout the 2000s significant advancements were made in computer vision and natural language processing by training machine learning models on'},\n",
       " {'kb': 'Big Data',\n",
       "  'content': \"in the past decade compute power and the size of data sets have continued to grow and machine learning has become capable of solving even more problems as a result today machine learning touches almost every part of our Lives sometimes we're well aware of it like when we interact with chat TPT in the browser or see a self-driving car go by but most of the time it's seamlessly woven into familiar experiences of our everyday life such as when we're approved for a new loan or get a catalog at home\"},\n",
       " {'kb': 'Increased awareness of ethical and responsible AI',\n",
       "  'content': \"this era has also been marked by an increased awareness of potential ethical issues in machine learning and by significant research in the field of responsible AI we want the benefits of AI but we also want AI that is responsible and doesn't amplify human bias\"}]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vectordb = 'aboutMLKBDemoDemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **嵌入式的技巧 - Embeddings**\n",
    "\n",
    "很多行业希望拥有 LLMs 的能力，希望 LLMs 能解决自己的企业内部问题。这就包括员工相关的内容如入职须知，请假和报销流程，还有福利查询等，企业业务流相关的内容包括相关文档，法规，执行流程等，也有一些面向客户的查询。虽然 LLMs 有强大的知识能力，但是基于行业的数据和知识是没办法获取的。那如何注入这些基于行业的知识内容呢？这也是让 LLMs 迈入企业化重要的一步。本章我们就会和大家讲讲如何注入行业的数据和知识，让 LLMs 变得更专业。也就是我们创建 RAG 应用的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore\n",
    "from semantic_kernel.memory.semantic_text_memory import SemanticTextMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_store = QdrantMemoryStore(vector_size=1536, url=\"http://localhost\",port=6333)\n",
    "await qdrant_store.create_collection(base_vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "memory = SemanticTextMemory(storage=qdrant_store, embeddings_generator=embedding_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('2c933d6b-6853-465f-a613-8d407db1fda5')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in kblist:\n",
    "    content = item[\"kb\"] + ' - ' + item[\"content\"]\n",
    "    id =str(uuid.uuid4())\n",
    "    await memory.save_information(base_vectordb, id=id, text=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"can you tell me what is different ML and AI\"\n",
    "\n",
    "memories = await memory.search(\n",
    "    base_vectordb, ask, limit=1, min_relevance_score=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Result: The difference between AI and ML - AI is a science of getting machines to accomplish tasks that typically require human level intelligence with score 0.88693666\n"
     ]
    }
   ],
   "source": [
    "result = ''\n",
    "for item in memories:\n",
    "    print(f\"Top Result: {item.text} with score {item.relevance}\")\n",
    "    result = item.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_plugin = kernel.import_plugin_from_prompt_directory(base_plugin , \"AnswerPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerFunc = answer_plugin[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = await kernel.invoke(answerFunc, sk.KernelArguments(input=result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "', such as understanding natural language or recognizing objects in images. It involves creating intelligent systems that can learn, reason, and make decisions on their own. On the other hand, ML is a subset of AI that focuses on the development of algorithms and models that enable computers to learn from and make predictions or decisions based on data. ML algorithms allow machines to improve their performance over time without being explicitly programmed. In summary, AI aims to replicate human intelligence in machines, while ML is a specific approach within AI that enables machines to learn and improve from data.'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "str(summary_result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
