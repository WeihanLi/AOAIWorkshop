{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic Kernel**\n",
    "\n",
    "Semantic Kernel 是一个轻量级的开源框架，通过 Semantic Kernel 您可以快速使用不同编程语言(C#/Python/Java)结合 LLMs(OpenAI、Azure OpenAI、Hugging Face 等模型) 构建智能应用。在我们进入生成式人工智能后，人机对话的方式有了很大的改变，我们用自然语言就可以完成与机器的对话，门槛降低了非常多。结合提示工程和大型语言模型，我们可以用更低的成本完成不同的业务。但如何把提示工程以及大模型引入到工程上？我们就需要一个像 Semantic Kernel 的框架作为开启智能大门的基础。在 2023 年 5 月，微软 CTO Kevin Scott 就提出了 Copilot Stack 的概念，人工智能编排就是核心。 Semantic Kernel 具备和 LLMs 以及各种提示工程/软件组成的插件组合的能力，因此也被看作 Copilot Stack 的最佳实践。通过 Semantic Kernel，你可以非常方便地构建基于 Copilot Stack 的解决方案，而且对于传统工程，也可以无缝对接。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Kernel 的特点**\n",
    "\n",
    "强大的插件 - 你可以通过结合自定义/预定义的插件解决智能业务的问题。让传统的代码和智能插件一起工作灵活地接入到应用场景，简化传统应用向智能化转型的过程。\n",
    "\n",
    "多模型支持 - 为您的智能应用配置“大脑”，可以是来自 Azure OpenAI Service , 也可以是 OpenAI ，以及来自 Hugging Face 上的各种离线模型。通过链接器你可以快速接入不同的“大脑”，让您的应用更智能更聪明。\n",
    "\n",
    "各式各样的链接器 - 链接器除了链接“大脑”外，还可以链接如向量数据库，各种商业软件，不同的业务中间件，让更多的业务场景进入智能成为可能\n",
    "\n",
    "开发便捷 - 简单易用，开发人员零成本入门"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Semantic Kernel 的缺点**\n",
    "\n",
    "毕竟 LLMs 还在不停发展，有很多新模型的加入，也有很多新的功能，以及新的概念引入。Semantic Kernel， LangChain 等开源框架都在努力适应这个新的摩尔定律，但版本的迭代会有不确定的更改。所以在使用的时候，开发者需要多留意对应 GitHub Repo 上的变更日志。\n",
    "\n",
    "还有 Semantic Kernel 需要兼顾多个编程语言，所以进度也是不一致，也会导致 Semantic Kernel 在不同技术栈人群的选择。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Semantic Kernel vs LangChain**\n",
    "\n",
    "我们没法不去作出一些客观的比较，毕竟 LangChain 拥有更多的使用群体。无可否认在落地场景上，LangChain 现在比 Semantic Kernel 更多，特别在入门参考例子上。我们来个更为全面的比较：\n",
    "\n",
    "LangChain 基于 Python 和 Javascript的开源框架，包含了众多预制组件，开发者可以无需多写提示工程就可以完成智能应用的开发。特别在复杂的应用场景，开发人员可以快速整合多个预定义的组件来综合完成。在开发角度，更适合具备数据科学或则人工智能基础的开发人员。\n",
    "\n",
    "Semantic Kernel 您可以基于 C#, Python, Java 的开源框架。更大的优势在工程化。毕竟它更像一个编程范式，传统开发人员可以很快掌握该框架进行应用开发，而且可以更好结合自定义的插件和提示工程完成企业的定制化业务智能化工作。\n",
    "\n",
    "两者有很多共通点，都还在版本迭代，我们需要基于团队结构，技术栈，应用场景作出选择。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: semantic-kernel in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (0.5.1.dev0)\n",
      "Requirement already satisfied: aiofiles<24.0.0,>=23.1.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (23.2.1)\n",
      "Requirement already satisfied: aiohttp<4.0,>=3.8 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (3.9.3)\n",
      "Requirement already satisfied: motor<4.0.0,>=3.3.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.24.2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (1.26.2)\n",
      "Requirement already satisfied: openai>=1.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (1.12.0)\n",
      "Requirement already satisfied: openapi_core<0.19.0,>=0.18.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (0.18.2)\n",
      "Requirement already satisfied: prance<24.0.0.0,>=23.6.21.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (23.6.21.0)\n",
      "Requirement already satisfied: pydantic>2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (2.6.2)\n",
      "Requirement already satisfied: python-dotenv==1.0.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (1.0.0)\n",
      "Requirement already satisfied: regex<2024.0.0,>=2023.6.3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from semantic-kernel) (2023.12.25)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (1.9.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from aiohttp<4.0,>=3.8->semantic-kernel) (4.0.3)\n",
      "Requirement already satisfied: pymongo<5,>=4.5 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from motor<4.0.0,>=3.3.1->semantic-kernel) (4.6.2)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (0.27.0)\n",
      "Requirement already satisfied: sniffio in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.66.2)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.7 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openai>=1.0->semantic-kernel) (4.9.0)\n",
      "Requirement already satisfied: asgiref<4.0.0,>=3.6.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (3.7.2)\n",
      "Requirement already satisfied: isodate in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.6.1)\n",
      "Requirement already satisfied: jsonschema<5.0.0,>=4.18.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (4.20.0)\n",
      "Requirement already satisfied: jsonschema-spec<0.3.0,>=0.2.3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.2.4)\n",
      "Requirement already satisfied: more-itertools in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (10.2.0)\n",
      "Requirement already satisfied: openapi-schema-validator<0.7.0,>=0.6.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.6.2)\n",
      "Requirement already satisfied: openapi-spec-validator<0.8.0,>=0.7.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.7.1)\n",
      "Requirement already satisfied: parse in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (1.20.1)\n",
      "Requirement already satisfied: werkzeug in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi_core<0.19.0,>=0.18.0->semantic-kernel) (3.0.1)\n",
      "Requirement already satisfied: chardet>=3.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (5.2.0)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.10 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (0.18.6)\n",
      "Requirement already satisfied: requests>=2.25 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (2.31.0)\n",
      "Requirement already satisfied: six~=1.15 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (1.16.0)\n",
      "Requirement already satisfied: packaging>=21.3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (23.2)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pydantic>2->semantic-kernel) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pydantic>2->semantic-kernel) (2.16.3)\n",
      "Requirement already satisfied: idna>=2.8 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel) (3.6)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from anyio<5,>=3.5.0->openai>=1.0->semantic-kernel) (1.2.0)\n",
      "Requirement already satisfied: certifi in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (1.0.4)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai>=1.0->semantic-kernel) (0.14.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from jsonschema<5.0.0,>=4.18.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.16.2)\n",
      "Requirement already satisfied: PyYAML>=5.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (6.0.1)\n",
      "Requirement already satisfied: pathable<0.5.0,>=0.4.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from jsonschema-spec<0.3.0,>=0.2.3->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.4.3)\n",
      "Requirement already satisfied: rfc3339-validator in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi-schema-validator<0.7.0,>=0.6.0->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.1.4)\n",
      "Requirement already satisfied: jsonschema-path<0.4.0,>=0.3.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (0.3.2)\n",
      "Requirement already satisfied: lazy-object-proxy<2.0.0,>=1.7.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from openapi-spec-validator<0.8.0,>=0.7.1->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (1.10.0)\n",
      "Requirement already satisfied: dnspython<3.0.0,>=1.16.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pymongo<5,>=4.5->motor<4.0.0,>=3.3.1->semantic-kernel) (2.6.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from requests>=2.25->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (2.1.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from ruamel.yaml>=0.17.10->prance<24.0.0.0,>=23.6.21.0->semantic-kernel) (0.2.8)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from werkzeug->openapi_core<0.19.0,>=0.18.0->semantic-kernel) (2.1.3)\n",
      "Requirement already satisfied: qdrant_client in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (1.7.3)\n",
      "Requirement already satisfied: grpcio>=1.41.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (1.62.0)\n",
      "Requirement already satisfied: grpcio-tools>=1.41.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (1.62.0)\n",
      "Requirement already satisfied: httpx>=0.14.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (0.27.0)\n",
      "Requirement already satisfied: numpy>=1.21 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (1.26.2)\n",
      "Requirement already satisfied: portalocker<3.0.0,>=2.7.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (2.8.2)\n",
      "Requirement already satisfied: pydantic>=1.10.8 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (2.6.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26.14 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from qdrant_client) (2.1.0)\n",
      "Requirement already satisfied: protobuf<5.0dev,>=4.21.6 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (4.25.3)\n",
      "Requirement already satisfied: setuptools in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from grpcio-tools>=1.41.0->qdrant_client) (68.2.2)\n",
      "Requirement already satisfied: anyio in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (4.2.0)\n",
      "Requirement already satisfied: certifi in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (2023.11.17)\n",
      "Requirement already satisfied: httpcore==1.* in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.0.4)\n",
      "Requirement already satisfied: idna in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (3.6)\n",
      "Requirement already satisfied: sniffio in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpcore==1.*->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (0.14.0)\n",
      "Requirement already satisfied: h2<5,>=3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from httpx[http2]>=0.14.0->qdrant_client) (4.1.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.16.3 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (2.16.3)\n",
      "Requirement already satisfied: typing-extensions>=4.6.1 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from pydantic>=1.10.8->qdrant_client) (4.9.0)\n",
      "Requirement already satisfied: hyperframe<7,>=6.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (6.0.1)\n",
      "Requirement already satisfied: hpack<5,>=4.0 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from h2<5,>=3->httpx[http2]>=0.14.0->qdrant_client) (4.0.0)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in /home/lokinfey/conda/envs/pydev/lib/python3.10/site-packages (from anyio->httpx>=0.14.0->httpx[http2]>=0.14.0->qdrant_client) (1.2.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install semantic-kernel -U\n",
    "! pip install qdrant_client -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import semantic_kernel as sk\n",
    "from dotenv import load_dotenv\n",
    "import semantic_kernel.connectors.ai.open_ai as skaoai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Semantic Kernel 中的 Kernel**\n",
    "\n",
    "如果把 Semantic Kernel 看作是 Copilot Stack 最佳实践，那 Kernel 就是 AI 编排的中心，在官方文档中也有所提及。通过 Kernel 可以和不同插件，服务，日志以及不同的模型链接在一起。所有 Semantic Kernel 的应用都从 Kernel 开始。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![sk_kernel](../../../imgs/ChatWithYourData/kernel.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = sk.Kernel()\n",
    "deployment, api_key, endpoint = sk.azure_openai_settings_from_dot_env()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Kernel(plugins=KernelPluginCollection(plugins={}), prompt_template_engine=PromptTemplateEngine(), memory=NullMemory(), text_completion_services={'azure_chat_competion_service': <function Kernel.add_text_completion_service.<locals>.<lambda> at 0x7f6b9e3028c0>}, chat_services={'azure_chat_competion_service': <function Kernel.add_chat_service.<locals>.<lambda> at 0x7f6b9e302830>}, text_embedding_generation_services={'embeddings_services': <function Kernel.add_text_embedding_generation_service.<locals>.<lambda> at 0x7f6b9e3036d0>}, default_text_completion_service='azure_chat_competion_service', default_chat_service='azure_chat_competion_service', default_text_embedding_generation_service='embeddings_services', retry_mechanism=PassThroughWithoutRetry(), function_invoking_handlers={}, function_invoked_handlers={})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kernel.add_chat_service(\"azure_chat_competion_service\", skaoai.AzureChatCompletion(\"GPT4Model\",endpoint,api_key=api_key,api_version = \"2023-12-01-preview\"))\n",
    "\n",
    "\n",
    "kernel.add_text_embedding_generation_service(\n",
    "        \"embeddings_services\", skaoai.AzureTextEmbedding(\"EmbeddingModel\", endpoint,api_key=api_key,api_version = \"2023-12-01-preview\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 Semantic Kernel 中，我们有不同的插件，用户可以使用预定义的插件，也可以使用自定义的插件。想了解更多可以关注下一章的内容，我们会详细讲述插件的使用。该例子，我们使用的是自定义插件，已经在 plugins 目录下了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_plugin = \"./plugins\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **什么是插件**\n",
    "\n",
    "Semantic Kernel 的一大特点是拥有强大的插件，通过结合自定义/预定义的插件解决智能业务的问题。让传统的代码和智能插件一起工作灵活地接入到应用场景简化传统应用向智能化转型的过程。\n",
    "\n",
    "我们知道 LLMs 本来的数据是有时间限制的，如果要增加实时内容或者企业化的知识是有相当大的缺陷。OpenAI 通过插件将 ChatGPT 连接到第三方应用程序。 这些插件使 ChatGPT 能够与开发人员定义的 API 进行交互，从而增强 ChatGPT 的功能并允许有更广泛的操作，如：\n",
    "\n",
    "检索实时信息，例如，体育赛事比分、股票价格、最新新闻等。\n",
    "\n",
    "检索知识库信息， 例如，公司文档、个人笔记等。\n",
    "\n",
    "协助用户进行相关操作，例如，预订航班、订餐等。\n",
    "\n",
    "Semantic Kernel 遵循 OpenAI 的插件的插件规范，可以很方便地接入和导出插件(如基于 Bing, Microsoft 365, OpenAI 的插件)，这样可以让开发人员很简单地调用不同的插件服务。除了兼容 OpenAI 的插件外，Semantic Kernel 内也有属于自己插件定义的方式。不仅可以在规定模版格式上定义 Plugins, 更可以在函数内定义 Plugins."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **通过模版定义插件**\n",
    "\n",
    "我们知道通过提示工程可以和 LLMs 进行对话。对于一个企业或者创业公司，我们在处理业务时，可能不是一个提示工程，可能需要有针对提示工程的合集。我们可以把这些针对业务能力的提示工程集放到 Semantic Kernel 的插件集合内。对于结合提示工程的插件，Semantic Kernel 有固定的模版，提示工程都放在 skprompt.txt 文件内，而相关参数设置都放在 config.json 文件内。最后的文件结构式这样的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "files_plugin = kernel.import_semantic_plugin_from_directory(base_plugin , \"FilePlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "nodes_files = os.listdir(\"./data/notes\")\n",
    "transcrips_files = os.listdir(\"./data/transcripts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kblist = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{ \"kb\": \"History of machine learning\", \"content\": \"The history of machine learning and artificial intelligence is marked by significant milestones. Notable discoveries include Bayes Theorem (1763, 1812), Least Square Theory (1805), Markov Chains (1913), Perceptron (1957), Nearest Neighbor (1967), Backpropagation (1970), and Recurrent Neural Networks (1982). The concept of a 'machine that can think' was introduced by Alan Turing in 1950. The Dartmouth Summer Research Project in 1956 coined the term 'artificial intelligence'. The period from 1956 to 1974 is known as 'The golden years' of AI. However, the complexity of creating intelligent machines led to an 'AI Winter' from 1974 to 1980. The 1980s saw the rise of expert systems, followed by an 'AI Chill' from 1987 to 1993 due to over-specialization. From 1993 to 2011, the field matured with the increase in data and compute power. Today, machine learning and AI are integral parts of our lives, raising important questions about privacy and freedom of expression.\" }\n",
      "{ \"kb\": \"Introduction to machine learning\", \"content\": \"This course introduces the basics of machine learning (ML), a subset of artificial intelligence (AI) that uses algorithms to find hidden patterns in data. The learning process of the human brain is compared to machine learning, with both involving the discovery of patterns and continuous learning. The course covers core concepts of ML, its history, fairness, regression, classification, clustering, natural language processing, time series forecasting, and reinforcement learning techniques. It also discusses real-world applications of ML, such as predicting disease likelihood, weather events, text sentiment, and detecting fake news. However, it does not cover deep learning, neural networks, and AI. The importance of understanding ML basics due to its widespread adoption is emphasized.\" }\n",
      "{ \"kb\": \"Techniques of Machine Learning\", \"content\": \"The process of machine learning involves several steps: deciding on the question, collecting and preparing data, choosing a training method, training the model, evaluating the model, parameter tuning, and prediction. Before building a model, tasks such as data collection and preparation, feature and target selection, data visualization, and dataset splitting are necessary. The model building process involves deciding on a training method, training the model using the training data, and evaluating the model's performance. Parameter tuning can be done to improve the model's performance. Finally, the model can be used to make predictions on new data.\" }\n"
     ]
    }
   ],
   "source": [
    "for f in nodes_files:\n",
    "    file = open(\"./data/notes/\"+f, \"r\") \n",
    "    content = file.read()\n",
    "    notesFunc = files_plugin[\"Notes\"]\n",
    "    result = await notesFunc(content)\n",
    "    print(result.result.replace(\"\\n\", \" \")) \n",
    "    json_result = json.loads(result.result)\n",
    "    kblist.append(json_result)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "{\"kb\": \"Introducing ML for Beginners\", \"content\": \"Open source curriculum, practical coding examples, step by step learning\"},\n",
      "{\"kb\": \"The difference between AI and ML\", \"content\": \"AI is task accomplishment, ML is AI subset, uses data learning algorithms\"},\n",
      "{\"kb\": \"What you'll learn in this course\", \"content\": \"Classical machine learning, core concepts, statistical techniques\"},\n",
      "{\"kb\": \"What you won't learn in this course\", \"content\": \"No data science, deep learning, neural networks, other AI techniques\"},\n",
      "{\"kb\": \"Why study Machine Learning\", \"content\": \"Solves complex problems, applicable in many fields, ubiquitous data use\"}\n",
      "]\n",
      "[\n",
      "{\"kb\": \"Alan Turing and the Turing test\", \"content\": \"Alan Turing, foundation, machine thinking, Turing test, intelligent computer\"},\n",
      "{\"kb\": \"The Dartmouth Summer Research Project on AI\", \"content\": \"Artificial intelligence, Dartmouth College, summer research, AI birth, research field\"},\n",
      "{\"kb\": \"The golden years of AI\", \"content\": \"AI optimism, problem solving, natural language processing, micro worlds, intelligent machines\"},\n",
      "{\"kb\": \"The AI winter\", \"content\": \"Complexity, understated promise, limited compute power, ethics questions, funding dried\"},\n",
      "{\"kb\": \"Resurgence and fall of AI for expert systems\", \"content\": \"Expert systems, AI optimism, practical applications, too specialized, personal computers\"},\n",
      "{\"kb\": \"Growth in AI driven by more data and more powerful hardware\", \"content\": \"Compute and storage capabilities, internet rise, smartphones, large data sets, machine learning\"},\n",
      "{\"kb\": \"Big Data\", \"content\": \"Compute power, data sets growth, machine learning, problem solving, everyday life\"},\n",
      "{\"kb\": \"Increased awareness of ethical and responsible AI\", \"content\": \"Ethical issues, machine learning, responsible AI, human bias, building using maintaining\"}\n",
      "]\n",
      "[\n",
      "{\"kb\": \"Decide if AI is the right approach\", \"content\": \"AI for data-driven, complex problems. Traditional programming for rule-based problems.\"},\n",
      "{\"kb\": \"Collect and prepare data\", \"content\": \"Collect, clean, normalize data. Decide input/output features. Split data into training and test sets.\"},\n",
      "{\"kb\": \"Train your model\", \"content\": \"Choose machine learning algorithm. Train model using training set.\"},\n",
      "{\"kb\": \"Evaluate your model\", \"content\": \"Test model using unseen test data. Ensure model generalizes well.\"},\n",
      "{\"kb\": \"Tune the hyperparameters\", \"content\": \"Choose good hyperparameters. Systematically search for best values.\"},\n",
      "{\"kb\": \"Test the model in the real world\", \"content\": \"Test model in intended context. Deploy model if successful.\"}\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "for f in transcrips_files:\n",
    "    file = open(\"./data/transcripts/\"+f, \"r\") \n",
    "    content = file.read()\n",
    "    transcripsFunc = files_plugin[\"Transcrips\"]\n",
    "    result = await transcripsFunc(content)\n",
    "    print(result.result)\n",
    "    json_result = json.loads(result.result)\n",
    "    for item in json_result:\n",
    "        kblist.append(item)\n",
    "    file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'kb': 'History of machine learning',\n",
       "  'content': \"The history of machine learning and artificial intelligence is marked by significant milestones. Notable discoveries include Bayes Theorem (1763, 1812), Least Square Theory (1805), Markov Chains (1913), Perceptron (1957), Nearest Neighbor (1967), Backpropagation (1970), and Recurrent Neural Networks (1982). The concept of a 'machine that can think' was introduced by Alan Turing in 1950. The Dartmouth Summer Research Project in 1956 coined the term 'artificial intelligence'. The period from 1956 to 1974 is known as 'The golden years' of AI. However, the complexity of creating intelligent machines led to an 'AI Winter' from 1974 to 1980. The 1980s saw the rise of expert systems, followed by an 'AI Chill' from 1987 to 1993 due to over-specialization. From 1993 to 2011, the field matured with the increase in data and compute power. Today, machine learning and AI are integral parts of our lives, raising important questions about privacy and freedom of expression.\"},\n",
       " {'kb': 'Introduction to machine learning',\n",
       "  'content': 'This course introduces the basics of machine learning (ML), a subset of artificial intelligence (AI) that uses algorithms to find hidden patterns in data. The learning process of the human brain is compared to machine learning, with both involving the discovery of patterns and continuous learning. The course covers core concepts of ML, its history, fairness, regression, classification, clustering, natural language processing, time series forecasting, and reinforcement learning techniques. It also discusses real-world applications of ML, such as predicting disease likelihood, weather events, text sentiment, and detecting fake news. However, it does not cover deep learning, neural networks, and AI. The importance of understanding ML basics due to its widespread adoption is emphasized.'},\n",
       " {'kb': 'Techniques of Machine Learning',\n",
       "  'content': \"The process of machine learning involves several steps: deciding on the question, collecting and preparing data, choosing a training method, training the model, evaluating the model, parameter tuning, and prediction. Before building a model, tasks such as data collection and preparation, feature and target selection, data visualization, and dataset splitting are necessary. The model building process involves deciding on a training method, training the model using the training data, and evaluating the model's performance. Parameter tuning can be done to improve the model's performance. Finally, the model can be used to make predictions on new data.\"},\n",
       " {'kb': 'Introducing ML for Beginners',\n",
       "  'content': 'Open source curriculum, practical coding examples, step by step learning'},\n",
       " {'kb': 'The difference between AI and ML',\n",
       "  'content': 'AI is task accomplishment, ML is AI subset, uses data learning algorithms'},\n",
       " {'kb': \"What you'll learn in this course\",\n",
       "  'content': 'Classical machine learning, core concepts, statistical techniques'},\n",
       " {'kb': \"What you won't learn in this course\",\n",
       "  'content': 'No data science, deep learning, neural networks, other AI techniques'},\n",
       " {'kb': 'Why study Machine Learning',\n",
       "  'content': 'Solves complex problems, applicable in many fields, ubiquitous data use'},\n",
       " {'kb': 'Alan Turing and the Turing test',\n",
       "  'content': 'Alan Turing, foundation, machine thinking, Turing test, intelligent computer'},\n",
       " {'kb': 'The Dartmouth Summer Research Project on AI',\n",
       "  'content': 'Artificial intelligence, Dartmouth College, summer research, AI birth, research field'},\n",
       " {'kb': 'The golden years of AI',\n",
       "  'content': 'AI optimism, problem solving, natural language processing, micro worlds, intelligent machines'},\n",
       " {'kb': 'The AI winter',\n",
       "  'content': 'Complexity, understated promise, limited compute power, ethics questions, funding dried'},\n",
       " {'kb': 'Resurgence and fall of AI for expert systems',\n",
       "  'content': 'Expert systems, AI optimism, practical applications, too specialized, personal computers'},\n",
       " {'kb': 'Growth in AI driven by more data and more powerful hardware',\n",
       "  'content': 'Compute and storage capabilities, internet rise, smartphones, large data sets, machine learning'},\n",
       " {'kb': 'Big Data',\n",
       "  'content': 'Compute power, data sets growth, machine learning, problem solving, everyday life'},\n",
       " {'kb': 'Increased awareness of ethical and responsible AI',\n",
       "  'content': 'Ethical issues, machine learning, responsible AI, human bias, building using maintaining'},\n",
       " {'kb': 'Decide if AI is the right approach',\n",
       "  'content': 'AI for data-driven, complex problems. Traditional programming for rule-based problems.'},\n",
       " {'kb': 'Collect and prepare data',\n",
       "  'content': 'Collect, clean, normalize data. Decide input/output features. Split data into training and test sets.'},\n",
       " {'kb': 'Train your model',\n",
       "  'content': 'Choose machine learning algorithm. Train model using training set.'},\n",
       " {'kb': 'Evaluate your model',\n",
       "  'content': 'Test model using unseen test data. Ensure model generalizes well.'},\n",
       " {'kb': 'Tune the hyperparameters',\n",
       "  'content': 'Choose good hyperparameters. Systematically search for best values.'},\n",
       " {'kb': 'Test the model in the real world',\n",
       "  'content': 'Test model in intended context. Deploy model if successful.'}]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kblist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_vectordb = 'aboutMLKBDemoDemo'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **嵌入式的技巧 - Embeddings**\n",
    "\n",
    "很多行业希望拥有 LLMs 的能力，希望 LLMs 能解决自己的企业内部问题。这就包括员工相关的内容如入职须知，请假和报销流程，还有福利查询等，企业业务流相关的内容包括相关文档，法规，执行流程等，也有一些面向客户的查询。虽然 LLMs 有强大的知识能力，但是基于行业的数据和知识是没办法获取的。那如何注入这些基于行业的知识内容呢？这也是让 LLMs 迈入企业化重要的一步。本章我们就会和大家讲讲如何注入行业的数据和知识，让 LLMs 变得更专业。也就是我们创建 RAG 应用的基础。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from semantic_kernel.connectors.memory.qdrant import QdrantMemoryStore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "qdrant_store = QdrantMemoryStore(vector_size=1536, url=\"http://localhost\",port=6333)\n",
    "await qdrant_store.create_collection(base_vectordb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel.register_memory_store(memory_store=qdrant_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UUID('c3a385bf-d170-43b0-add3-52dafa5bb195')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uuid.uuid4()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in kblist:\n",
    "    content = item[\"kb\"] + ' - ' + item[\"content\"]\n",
    "    id =str(uuid.uuid4())\n",
    "    await kernel.memory.save_information(base_vectordb, id=id, text=content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ask = \"can you tell me what is different ML and AI\"\n",
    "\n",
    "memories = await kernel.memory.search(\n",
    "    base_vectordb, ask, limit=1, min_relevance_score=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top Result: The difference between AI and ML - AI is task accomplishment, ML is AI subset, uses data learning algorithms with score 0.87940943\n"
     ]
    }
   ],
   "source": [
    "result = ''\n",
    "for memory in memories:\n",
    "    print(f\"Top Result: {memory.text} with score {memory.relevance}\")\n",
    "    result = memory.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_plugin = kernel.import_semantic_plugin_from_directory(base_plugin , \"AnswerPlugin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "answerFunc = answer_plugin[\"Summary\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_result = await answerFunc(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"AI, or Artificial Intelligence, is about enabling machines to perform tasks that would normally require human intelligence. On the other hand, Machine Learning (ML) is a subset of AI that focuses on using data and algorithms to enable machines to learn and improve from experience. In simple terms, AI is about smart task completion, while ML is about learning from data to improve AI's performance.\""
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary_result.result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pydev",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
